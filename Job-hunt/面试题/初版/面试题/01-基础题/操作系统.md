# ==操作系统==

## 进程、线程、协程

- 进程

  - 进程是具有一定独立功能的程序关于某个数据集合上的一次运行活动。进程是系统进行资源分配和调度的独立单位。每个进程都有自己独立的内存空间。由于进程比较重量，占据独立的内存，所以进程切换的开销（栈，寄存器，虚拟内存，文件句柄）比较大，但是相对比较稳定安全。

  - 进程内存空间的组成包括：

    - 用户栈：保存进程需要使用的各种临时数据，自顶向下扩展
    - 代码库：进程执行有时需要依赖共享的代码库，这些代码库被映射到用户栈下方的虚地址处，标记为只读。
    - 用户堆：堆管理的是进程动态分配的内存。字下向上扩展。
    - 数据与代码段：他们原本保存在进程需要执行的 二进制文件中，进程执行前，操作系统将他们载入虚拟地址中。数据段都是全局变量
    - 内核部分：位于进程地址空间的最顶端，只有当进程进入内核态时，才能访问内核内存，内核部分也有代码段与数据段。

  - 进程的上下包括哪些：

    - 堆栈
    - 寄存器
    - 代码和数据
    - 内存

- 线程

  - 线程是进程的一个实体，是CPU调度和分派的基本单位，是比进程更小的能独立运行的基本单位。线程自己基本上不拥有系统资源，只拥有一点在运行中必不可少的资源（如程序计数器，一组寄存器和栈），但是线程可与同属一个进程的其他线程共享进程所拥有的全部资源。线程之间的通信主要通过共享内存，上下文切换很快，资源开销较少，但是相对于进程不够稳定容易丢失数据。
  - 线程之间有独立的寄存器、栈但是共享数据、代码段和堆
  - 提高吞吐量思路二
    - 为了降低线程的创建开销，可以使用线程池技术，理论上线程池越大，则吞吐越高，但线程池越大，CPU花在切换上的开销也越大（性能不高
    - 使用异步非阻塞的开发模型，用一个进程或线程接收请求，然后通过 IO 多路复用让进程或线程不阻塞，省去上下文切换的开销（实现复杂

- 协程

  - 协程一种用户态的轻量级线程，协程的调度完全由用户控制。协程拥有自己的寄存器上下文和栈。协程调度切换时，将寄存器上下文和栈保存到其他地方。在切回来的时候，恢复先前保存的寄存器上下文和栈，直接操作栈则基本没有内核切换的开销，可以不加锁的访问全局变量，所以上下文的切换非常快。

  - **协程并没有增加线程数量，只是在线程的基础之上通过分时复用的方式运行多个协程**，而且协程的切换在用户态完成，对os来说是 不可见的，切换的代价比线程从用户态到内核态的代价小很多

  - **线程栈空间通常是MB级别， 协程栈空间最小KB级别**

  - 使用

    - 在有大量IO操作业务的情况下，我们采用协程替换线程，可以到达很好的效果，一是降低了系统内存，二是减少了系统切换开销，因此系统的性能也会提升。

    - **在协程调用阻塞IO操作的时候，操作系统会让线程进入阻塞状态，当前的协程和其它绑定在该线程之上的协程都会陷入阻塞而得不到调度，这往往是不能接受的**

    - 在协程中尽量不要调用阻塞IO的方法，比如打印，读取文件，Socket接口等，除非改为异步调用的方式，并且协程只有在IO密集型的任务中才会发挥作用。

      **协程只有和异步IO结合起来才能发挥出最大的威力**

    - 传统的生产者-消费者模型是一个线程写消息，一个线程取消息，通过锁机制控制队列和等待，但一不小心就可能死锁。
      如果改用协程，生产者生产消息后，直接通过yield跳转到消费者开始执行，待消费者执行完毕后，切换回生产者继续生产，效率极高

    - jdk19支持虚拟线程，也就是协程


## 虚拟地址空间

- 在多任务操作系统中，每个 进程都运行在属于自己的内存沙盘中，这个沙盘就是虚拟地址空间（virtual address space）。虚拟地址空间由内核空间（kernel space）和用户模式空间（user mode space）两部分组成。
  虚拟地址会通过页表（page table）映射到物理内存，页表由操作系统维护并被处理器引用，每个进程都有自己的页表。内核空间在页表中拥有较高特权级，因此用户态程序试图访问这些页是会导致一个页错误（page fault）。其中内核空间是持续存在的，并且在所有进程中都映射到同样的物理内存。与此相反，用户模式空间的                                                                                                                                                                                                                                                                                                                                                             映射随进程切换的发生而不断变化。                  

- 虚拟地址空间中用户区地址范围是 0~3G，里边分为多个区块：
    

  - **保留区**: 位于虚拟地址空间的最底部，未赋予物理地址。任何对它的引用都是非法的，程序中的空指针（NULL）指向的就是这块内存地址。

  - **.text段**: 代码段也称正文段或文本段，通常用于存放程序的执行代码 (即 CPU 执行的机器指令)，代码段一般情况下是只读的，这是对执行代码的一种保护机制。

  - **.data段**: 数据段通常用于存放程序中已初始化且初值不为 0 的全局变量和静态变量。数据段属于静态内存分配 (静态存储区)，可读可写。

  - .**bss段**: 未初始化以及初始为 0 的全局变量和静态变量，操作系统会将这些未初始化的变量初始化为 0

  - **堆(heap)**：用于存放进程运行时动态分配的内存。

    - - 堆中内容是匿名的，不能按名字直接访问，只能通过指针间接访问。
           - 堆向高地址扩展 (即 “向上生长”)，是不连续的内存区域。这是由于系统用链表来存储空闲内存地址，自然不连续，而链表从低地址向高地址遍历。

    - **内存映射区(mmap)**：作为内存映射区加载磁盘文件，或者加载程序运作过程中需要调用的动态库。

    - **栈(stack)**: 存储函数内部声明的非静态局部变量，函数参数，函数返回地址等信息，栈内存由编译器自动分配释放。栈和堆相反地址 “向下生长”，分配的内存是连续的。

    - **命令行参数**：存储进程执行的时候传递给 main() 函数的参数，argc，argv []，env[]

    - **环境变量**: 存储和进行相关的环境变量，比如：工作路径，进程所有者等信息

- 理解
  1、创建一个进程时，操作系统会为该进程分配一个 4GB 大小的虚拟 进程地址空间。之所以是 4GB ，是因为在 32 位的操作系统中，一个指针长度是 4 字节，而 4 字节指针的寻址能力是从 0x00000000~0xFFFFFFFF ，最大值 0xFFFFFFFF 表示的即为 4GB 大小的容量。
    2、每个进程只能访问自己虚拟地址空间中的数据，无法访问别的进程中的数据，通过这种方法实现了进程间的地址隔离。
    3、4GB 的虚拟地址被分成了 4 部分： NULL 指针区、用户区、 64KB 禁入区、内核区。应用程序能使用的只是用户区而已，大约 2GB 左右 ( 最大可以调整到 3GB) 。内核区为 2GB ，内核区保存的是系统线程调度、内存管理、设备驱动等数据，这部分数据供所有的进程共享，但应用程序是不能直接访问的。
    4、目前PC上页面大小为4K，程序运行到哪页就为哪页分配内存，并建立虚拟地址空间页和刚刚建立的物理内存页间的映射。
    5、一个PE是一些编译好的数据和指令的集合，也被分为很多页。4G的地址空间并非真正创建，只是创建虚拟地址空间到物理地址空间映射的页表。
    6、PE文件中所有的段会一一映射到虚拟地址空间中相应的页。
    7、当CPU访问某一个虚拟地址时，发现该地址并没有相关联的物理地址时，产生一个缺页错误，于是CPU将控制权交回OS，OS为该页面映射物理内存。
    8、当CPU要访问的数据在内存中时，可以直接访问。
    当CPU要访问的数据不再内存中，而位于页交换文件中，OS会在内存中找到一个闲置的页面，如果找不到闲置的页面，先释放一个已分配的页面。
    9、页交换文件的左右时物理内存不够时，将部分数据从内存中移到页交换文件中，否则要释放物理内存，删除页面映射关系。

- 为什么虚拟地址空间切换会比较耗时？

  进程都有自己的虚拟地址空间，把虚拟地址转换为物理地址需要查找页表，页表查找是一个很慢的过程，因此通常使用Cache来缓存常用的地址映射，这样可以加速页表查找，这个Cache就是TLB（translation Lookaside Buffer，TLB本质上就是一个Cache，是用来加速页表查找的）。

  由于每个进程都有自己的虚拟地址空间，那么显然每个进程都有自己的页表，那么**当进程切换后页表也要进行切换，页表切换后TLB就失效了**，Cache失效导致命中率降低，那么虚拟地址转换为物理地址就会变慢，表现出来的就是程序运行会变慢，而线程切换则不会导致TLB失效，因为线程无需切换地址空间，因此我们通常说线程切换要比较进程切换快

## 虚拟内存

- 虚拟内存是内存管理的一种方式， 它将多个物理内存碎片和部分磁盘空间重定义为连续的地址空间，以此让程序认为自己拥有连续可用的内存。它在磁盘上划分出一块空间由操作系统管理，当物理内存耗尽是充当物理内存来使用。当物理内存不足时，操作系统会将处于不活动状态的程序以及它们的数据全部交换到磁盘上来释放物理内存，以供其它程序使用。

## 进程间通信方式

- 管道：管道这种通讯方式有两种限制，一是半双工的通信，数据只能单向流动，二是只能在具有亲缘关系的进程间使用。进程的亲缘关系通常是指父子进程关系。

  管道可以分为两类：匿名管道和命名管道。匿名管道是单向的，只能在有亲缘关系的进程间通信；命名管道以磁盘文件的方式存在，可以实现本机任意两个进程通信。

  两个进程间交换数据的通道（gong'xian

- 信号 ： 信号是一种比较复杂的通信方式，于通知接收进程某个事件已经发生。信号可以在任何时候发给某一进程，而无需知道该进程的状态。

  >  **Linux系统中常用信号**：
  >  （1）**SIGHUP**：用户从终端注销，所有已启动进程都将收到该进程。系统缺省状态下对该信号的处理是终止进程。
  >
  >  （2）**SIGINT**：程序终止信号。程序运行过程中，按`Ctrl+C`键将产生该信号。
  >
  >  （3）**SIGQUIT**：程序退出信号。程序运行过程中，按`Ctrl+\\`键将产生该信号。
  >
  >  （4）**SIGBUS和SIGSEGV**：进程访问非法地址。
  >
  >  （5）**SIGFPE**：运算中出现致命错误，如除零操作、数据溢出等。
  >
  >  （6）**SIGKILL**：用户终止进程执行信号。shell下执行`kill -9`发送该信号。
  >
  >  （7）**SIGTERM**：结束进程信号。shell下执行`kill 进程pid`发送该信号。
  >
  >  （8）**SIGALRM**：定时器信号。
  >
  >  （9）**SIGCLD**：子进程退出信号。如果其父进程没有忽略该信号也没有处理该信号，则子进程退出后将形成僵尸进程。

- 信号量：信号量是一个**计数器**，可以用来控制多个进程对共享资源的访问。它常作为一种**锁机制**，防止某进程正在访问共享资源时，其他进程也访问该资源。因此，主要作为进程间以及同一进程内不同线程之间的同步手段。

- 消息队列：消息队列是消息的链接表，存放在内核中并由消息队列标识符标识，包括Posix消息队列和System V消息队列。有足够权限的进程可以向队列中添加消息，被赋予读权限的进程则可以读走队列中的消息。消息队列克服了信号承载信息量少，管道只能承载无格式字节流以及缓冲区大小受限等缺点。

- 共享内存：共享内存就是映射一段能被其他进程所访问的内存，这段共享内存由一个进程创建，但多个进程都可以访问。共享内存是最快的 IPC 方式，它是针对其他进程间通信方式运行效率低而专门设计的。它往往与其他通信机制，如信号量，配合使用，来实现进程间的同步和通信。

- Socket：与其他通信机制不同的是，它可用于不同机器间的进程通信。

**优缺点**：

* 管道：速度慢，容量有限；

* Socket：任何进程间都能通讯，但速度慢；

* 消息队列：容量受到系统限制，且要注意第一次读的时候，要考虑上一次没有读完数据的问题；

* 信号量：不能传递复杂消息，只能用来同步；

* 共享内存区：能够很容易控制容量，速度快，但要保持同步，比如一个进程在写的时候，另一个进程要注意读写的问题，相当于线程中的线程安全，当然，共享内存区同样可以用作线程间通讯，不过没这个必要，线程间本来就已经共享了同一进程内的一块内存。

## 进程间同步方式

1、临界区：通过对多线程的串行化来访问公共资源或一段代码，速度快，适合控制数据访问。

优点：保证在某一时刻只有一个线程能访问数据的简便办法。

缺点：虽然临界区同步速度很快，但却只能用来同步本进程内的线程，而不可用来同步多个进程中的线程。

2、互斥量：为协调共同对一个共享资源的单独访问而设计的。互斥量跟临界区很相似，比临界区复杂，互斥对象只有一个，只有拥有互斥对象的线程才具有访问资源的权限。

优点：使用互斥不仅仅能够在同一应用程序不同线程中实现资源的安全共享，而且可以在不同应用程序的线程之间实现对资源的安全共享。

缺点：

* 互斥量是可以命名的，也就是说它可以跨越进程使用，所以创建互斥量需要的资源更多，所以如果只为了在进程内部是用的话使用临界区会带来速度上的优势并能够减少资源占用量。

* 通过互斥量可以指定资源被独占的方式使用，但如果有下面一种情况通过互斥量就无法处理，比如现在一位用户购买了一份三个并发访问许可的数据库系统，可以根据用户购买的访问许可数量来决定有多少个线程/进程能同时进行数据库操作，这时候如果利用互斥量就没有办法完成这个要求，信号量对象可以说是一种资源计数器。

3、信号量：为控制一个具有有限数量用户资源而设计。它允许多个线程在同一时刻访问同一资源，但是需要限制在同一时刻访问此资源的最大线程数目。互斥量是信号量的一种特殊情况，当信号量的最大资源数=1就是互斥量了。

优点：适用于对Socket（套接字）程序中线程的同步。

缺点:

* 信号量机制必须有公共内存，不能用于分布式操作系统，这是它最大的弱点；

* 信号量机制功能强大，但使用时对信号量的操作分散， 而且难以控制，读写和维护都很困难，加重了程序员的编码负担；

* 核心操作P-V分散在各用户程序的代码中，不易控制和管理，一旦错误，后果严重，且不易发现和纠正。

4、事件： 用来通知线程有一些事件已发生，从而启动后继任务的开始。

优点：事件对象通过通知操作的方式来保持线程的同步，并且可以实现不同进程中的线程同步操作。

##  临界区冲突

每个进程中访问临界资源的那段程序称为临界区，**一次仅允许一个进程使用的资源称为临界资源。**

解决冲突的办法：

- 如果有若干进程要求进入空闲的临界区，**一次仅允许一个进程进入**，如已有进程进入自己的临界区，则其它所有试图进入临界区的进程必须等待；
- 进入临界区的进程要在**有限时间内退出**。
- 如果进程不能进入自己的临界区，则应**让出CPU**，避免进程出现“忙等”现象。

## 死锁

**什么是死锁**：

在两个或者多个并发进程中，如果每个进程持有某种资源而又等待其它进程释放它或它们现在保持着的资源，在未改变这种状态之前都不能向前推进，称这一组进程产生了死锁。通俗的讲就是两个或多个进程无限期的阻塞、相互等待的一种状态。

**死锁产生的四个必要条件**：（有一个条件不成立，则不会产生死锁）

- 互斥条件：一个资源一次只能被一个进程使用
- 请求与保持条件：一个进程因请求资源而阻塞时，对已获得资源保持不放
- 不剥夺条件：进程获得的资源，在未完全使用完之前，不能强行剥夺
- 循环等待条件：若干进程之间形成一种头尾相接的环形等待资源关系



## **处理死锁**

常用的处理死锁的方法有：死锁预防、死锁避免、死锁检测、死锁解除、鸵鸟策略。

**（1）死锁的预防：**基本思想就是确保死锁发生的四个必要条件中至少有一个不成立：

还没运行前预防

> - ① 破除资源互斥条件
> - ② 破除“请求与保持”条件：实行资源预分配策略，进程在运行之前，必须一次性获取所有的资源。缺点：在很多情况下，无法预知进程执行前所需的全部资源，因为进程是动态执行的，同时也会降低资源利用率，导致降低了进程的并发性。没有拿到则放弃
> - ③ 破除“不可剥夺”条件：允许进程强行从占有者那里夺取某些资源。当一个已经保持了某些不可被抢占资源的进程，提出新的资源请求而不能得到满足时，它必须释放已经保持的所有资源，待以后需要时再重新申请。这意味着进程已经占有的资源会被暂时被释放，或者说被抢占了。
> - ④ 破除“循环等待”条件：实行资源有序分配策略，对所有资源排序编号，按照顺序获取资源，将紧缺的，稀少的采用较大的编号，在申请资源时必须按照编号的顺序进行，一个进程只有获得较小编号的进程才能申请较大编号的进程。

**（2）死锁避免：**

将要运行，根据当前资源避免

死锁预防通过约束资源请求，防止4个必要条件中至少一个的发生，可以通过直接或间接预防方法，但是都会导致低效的资源使用和低效的进程执行。而死锁避免则允许前三个必要条件，但是通过动态地检测资源分配状态，以确保循环等待条件不成立，从而确保系统处于安全状态。所谓安全状态是指：如果系统能按某个顺序为每个进程分配资源（不超过其最大值），那么系统状态是安全的，换句话说就是，如果存在一个安全序列，那么系统处于安全状态。**银行家算法**是经典的死锁避免的算法。

**（3）死锁检测：**

运行时检测

死锁预防策略是非常保守的，他们通过限制访问资源和在进程上强加约束来解决死锁的问题。死锁检测则是完全相反，它不限制资源访问或约束进程行为，只要有可能，被请求的资源就被授权给进程。但是操作系统会周期性地执行一个算法检测前面的循环等待的条件。死锁检测算法是通过资源分配图来检测是否存在环来实现，从一个节点出发进行深度优先搜索，对访问过的节点进行标记，如果访问了已经标记的节点，就表示有存在环，也就是检测到死锁的发生。

> - （1）如果进程-资源分配图中无环路，此时系统没有死锁。 
> - （2）如果进程-资源分配图中有环路，且每个资源类中只有一个资源，则系统发生死锁。 
> - （3）如果进程-资源分配图中有环路，且所涉及的资源类有多个资源，则不一定会发生死锁。

**（4）死锁解除：**

发生死锁后解除

​	死锁解除的常用方法就是终止进程和资源抢占，回滚。所谓进程终止就是简单地终止一个或多个进程以打破循环等待，包括两种方式：	终止所有死锁进程和一次只终止一个进程直到取消死锁循环为止；所谓资源抢占就是从一个或者多个死锁进程那里抢占一个或多个资源。

**（5）鸵鸟策略：**

​	把头埋在沙子里，假装根本没发生问题。因为解决死锁问题的代价很高，因此鸵鸟策略这种不采取任何措施的方案会获得更高的性能。	当发生死锁时不会对用户造成多大影响，或发生死锁的概率很低，可以采用鸵鸟策略。大多数操作系统，包括 Unix，Linux 和 			Windows，处理死锁问题的办法仅仅是忽略它。

## 进程调度策略

* **先来先服务**：非抢占式的调度算法，按照请求的顺序进行调度。有利于长作业，但不利于短作业，因为短作业必须一直等待前面的长作业执行完毕才能执行，而长作业又需要执行很长时间，造成了短作业等待时间过长。另外，对`I/O`密集型进程也不利，因为这种进程每次进行`I/O`操作之后又得重新排队。

* **短作业优先**：非抢占式的调度算法，按估计运行时间最短的顺序进行调度。长作业有可能会饿死，处于一直等待短作业执行完毕的状态。因为如果一直有短作业到来，那么长作业永远得不到调度。

* **最短剩余时间优先**：最短作业优先的抢占式版本，按剩余运行时间的顺序进行调度。 当一个新的作业到达时，其整个运行时间与当前进程的剩余时间作比较。如果新的进程需要的时间更少，则挂起当前进程，运行新的进程。否则新的进程等待。

* **时间片轮转**：将所有就绪进程按 `FCFS` 的原则排成一个队列，每次调度时，把 `CPU` 时间分配给队首进程，该进程可以执行一个时间片。当时间片用完时，由计时器发出时钟中断，调度程序便停止该进程的执行，并将它送往就绪队列的末尾，同时继续把 `CPU` 时间分配给队首的进程。

  时间片轮转算法的效率和时间片的大小有很大关系：因为进程切换都要保存进程的信息并且载入新进程的信息，如果时间片太小，会导致进程切换得太频繁，在进程切换上就会花过多时间。 而如果时间片过长，那么实时性就不能得到保证。 

* **优先级调度**：为每个进程分配一个优先级，按优先级进行调度。为了防止低优先级的进程永远等不到调度，可以随着时间的推移增加等待进程的优先级。

* **响应比优先调度**
  Hansen针对短进程优先调度算法的缺点提出了最高响应比优先调度算法。这是一个非剥夺的算法。按照此算法，每个进程都有一个动态优先数，该优先数不但是要求的服务时间的函数，而且是该进程得到服务所花费的等待时间的函数。进程的动态优先数计算公式如下：
  优先数=（等待时间+要求的服务时间）/要求的服务时间

* **多级反馈队列调度**

  多级反馈队列算法，不必事先知道各种进程所需要执行的时间，他是当前被公认的一种较好的进程调度算法。其实施过程如下：

  　　1)设置多个就绪队列，并为各个队列赋予不同的优先级。在优先权越高的队列中，为每个进程所规定的执行时间片就越小。

  　　2)当一个新进程进入内存后，首先放入第一队列的末尾，按照先来先去原则排队等候调度。如果他能在一个时间片中完成，便可撤离；如果未完成，就转入第二队列的末尾，同样等待调度.....如此下去，当一个长作业（进程）从第一队列依次将到第n队列（最后队列）后，便按第n队列时间片轮转运行。

  　　3)仅当第一队列空闲的时候，调度程序才调度第二队列中的进程运行；仅当第1到（i-1）队列空时，才会调度第i队列中的进程运行，并执行相应的时间片轮转。

  　　4)如果处理机正在处理第i队列中某进程，又有新进程进入优先权较高的队列，则此新队列抢占正在运行的处理机，并把正在运行的进程放在第i队列的队尾。

## 进程状态

进程一共有`5`种状态，分别是创建、就绪、运行（执行）、终止、阻塞。 

![进程状态](B:/我的坚果云/面试题/基础题/images/进程状态.png)

- 运行状态就是进程正在`CPU`上运行。在单处理机环境下，每一时刻最多只有一个进程处于运行状态。 
- 就绪状态就是说进程已处于准备运行的状态，即进程获得了除`CPU`之外的一切所需资源，一旦得到`CPU`即可运行。 
- 阻塞状态就是进程正在等待某一事件而暂停运行，比如等待某资源为可用或等待`I/O`完成。即使`CPU`空闲，该进程也不能运行。 

**运行态→阻塞态**：往往是由于等待外设，等待主存等资源分配或等待人工干预而引起的。
**阻塞态→就绪态**：则是等待的条件已满足，只需分配到处理器后就能运行。（就差获取时间片就可以运行
**运行态→就绪态**：不是由于自身原因，而是由外界原因使运行状态的进程让出处理器，这时候就变成就绪态。例如时间片用完，或有更高优先级的进程来抢占处理器等。
**就绪态→运行态**：系统按某种策略选中就绪队列中的一个进程占用处理器，此时就变成了运行态。

## 分页、分段

- 分页

  为了管理内存空间和提高碎片内存的利用率

  把内存空间划分为**大小相等且固定的块**，作为主存的基本单位。因为程序数据存储在不同的页面中，而页面又离散的分布在内存中，**因此需要一个页表来记录映射关系，以实现从页号到物理块号的映射。**

  访问分页系统中内存数据需要**两次的内存访问** (一次是从内存中访问页表，从中找到指定的物理块号，加上页内偏移得到实际物理地址；第二次就是根据第一次得到的物理地址访问内存取出数据)。

![页表](B:/我的坚果云/面试题/基础题/images/页表.png)

- 分段

  **分页是为了提高内存利用率，而分段是为了满足程序员在编写代码的时候的一些逻辑需求(比如数据共享，数据保护，动态链接等)。**

  分段内存管理当中，**地址是二维的，一维是段号，二维是段内地址；其中每个段的长度是不一样的，而且每个段内部都是从0开始编址的**。由于分段管理中，每个段内部是连续内存分配，但是段和段之间是离散分配的，因此也存在一个逻辑地址到物理地址的映射关系，相应的就是段表机制。

![段表](B:/我的坚果云/面试题/基础题/images/段表.png)

- 区别
  - 分页对程序员是透明的，但是分段需要程序员显式划分每个段。 
  - 分页的地址空间是一维地址空间，分段是二维的。 
  - 页的大小不可变，段的大小可以动态改变。 
  - 分页主要用于实现虚拟内存，从而获得更大的地址空间；分段主要是为了使程序和数据可以被划分为逻辑上独立的地址空间并且有助于共享和保护。

## 局部页面置换算法

- 功能：当缺页中断发生，需要调入新的页面，而内存已满时，选择内存当中哪个物理页面被置换

- 目标：尽可能地减少页面的换进换出次数（即缺页中断的次数），具体来说，把未来不再使用的或短期内较少使用的页面换出，通常只能在局部性原理指导下，依据过去的统计数据来进行预测

  缺页情况越少，性能越好

- 局部页面置换算法

  - 随机法

  - **最优页面置换算法（OPT）**

    - 基本思路 :
      -  当一个缺页中断发生时，对于保存在内存当中的每一个逻辑页面，计算在它的下一次访问之前，还需等待多长时间，从中选择等待时间最长的那个，作为被置换的页面

    这是一种理想情况，在实际系统中是无法实现的，因为操作系统无法知道每一个页面要等待多长时间以后才会再次被访问

    可用作其他算法的性能评价的依据，在一个模拟器上运行某个程序，并记录每一次的页面访问情况，在第二遍运行时即可使用最优算法

  - **先进先出算法（FIFO）**

    - 基本思路
      - 淘汰在内存中驻留时间最长的页面，具体来说，系统维护着一个链表，记录了所有位于内存当中的逻辑页面，从链表的排列顺序来看，链首页面的驻留时间最长，链尾页面的驻留时间最短，当发生一个缺页中断时， 把链首页面淘汰出去，并把新的页面添加到链表的末尾

    - 性能较差，调出的页面有可能是经常要访问的页面，并且有 Belady 现象，FIFO算法很少单独使用

  - **最近最久未使用算法（LRU）**

    - 基本思路：
      - 当一个缺页中断发生时，淘汰最久未使用的那个页面

    它是对 OPT 算法的一个近似，依据是程序的局部性原理，即在最近一小段时间（最近几条指令）内，如果某些页面被频繁地访问，那么再将来的一小段时间内，它们还可能会再一次被频繁地访问，反过来说，如果过去某些页面长时间未被访问，那么在将来它们还可能会长时间地得不到访问

    需要记录各个页面使用时间的先后顺序，开销比较大

    链表实现：系统维护一个页面链表，最近刚刚使用过的页面作为首节点，最久未使用的作为尾结点，再一次访问内存时，找出相应的页面，把它从链表中摘下来，再移动到链表首，每次缺页中断发生时，淘汰链表末尾的页面

    栈实现：设置一个活动页面栈，当访问某页时，将此页号压入栈顶，然后，考察栈内是否有与此页面相同的页号，若有则抽出，当需要淘汰一个页面时，总是选择栈底的页面，它就是最久未使用的

  - **时钟页面置换算法（Clock）**

    - 基本思路 :

      - 1表示刚被访问
      - 一个环形结构，初始都是0（满了则全是1），表示为空，每进来一个，发现0，则在相应位置该为1。并移动到下一位
      - 若发现是1，则改为0（下次轮转到时必然会发现），继续寻找下一个为0的位置。
      - 若命中某个页面，若发现其位置为0，则也改为1，让其再存活一次

      

      **所有的页面都保存在⼀个类似钟面的「环形链表」中**

      

      需要用到页表项的访问位，当一个页面被装入内存时，把该位初始化为 0，然后如果这个页面被访问，则把该位置设为 1

      把各个页面组织成环形链表（类似钟表面），把指针指向最老的页面（最先进来）

      当发生一个缺页中断时，考察指针所指向的最老页面，若它的访问位为 0，立即淘汰；若访问位为 1，然后指针往下移动一格，如此类推，直到找到被淘汰的页面，然后把指针移动到下一格

    - **它跟 LRU 近似，⼜是对 FIFO 的⼀种改进**

      - 0表示可以淘汰，1则是可以存活一次，访问到则会修改为1，表示最近使用的不会马上被淘汰，是有点LRU

    - 同时找到最近的0页面，则是有点FIFO

    - 改进

      - 将1优先级提高到4
      - 因此在时钟置换算法的基础上可以做一个改进，就是增加一个标记为m，修改过标记为1，没有修改过则标记为0。那么u和m组成了一个元组，有四种可能，其被逐出的优先顺序也不一样：
        - (u=0, m=0) 没有使用也没有修改，被逐出的优先级最高；
        - (u=1, m=0) 使用过，但是没有修改过，优先级第二；
        - (u=0, m=1) 没有使用过，但是修改过，优先级第三；
        - (u=1, m=1) 使用过也修改过，优先级第四

    - 流程 :

      如果访问页在物理内存中，访问位设置为 1

      如果不在物理页，从指针当前指向的物理页开始，如果访问位为 0，替换当前页，指针指向下一个物理页；如果访问位为 1，设置为 0 后，访问下一个物理页再进行判断

      如果所有物理页的访问位都被设置为 0，即指针又回到第一次指向的物理页，将其进行替换

  - **二次机会法**

    ​	Clock 有时候会把一些修改位为 1（有过写操作）的页面进行置换,，这样代价会比较大

    ​	改进 Clock，可以结合访问位、修改位来决定应该置换哪一页

    ​	访问位、修改位决定，替换的优先级不同：没有读也没写过，那么直接替换,，如果写过或读过，允许跳过一次，如果即写过又读过，允许跳过两次

  - **最不常用算法（LFU）**

    ​	基本思路：当一个缺页中断发生时，淘汰访问次数最少的那个页面

    ​	实现方法：对每一个页面设置一个访问计数器，每当一个页面被访问时，该页面的访问计数器加 1，当发生缺页中断时，淘汰计数值最小的那个页面

- LRU、FIFO、Clock

  - LRU、FIFO 本质都是先进先出的思路

    ​	LRU：依据页面的最近访问时间排序，需要动态地调整顺序，算法性能较好，但系统开销较大

    ​	FIFO：依据页面进入内存的时间排序，FIFO 的页面进入时间是固定不变的，算法系统开销较小，会发生 Belady 现象

  - LRU 可退化成 FIFO

    ​	如页面进入内存后没有被访问，最近访问时间与进入内存的时间相同

    ​	例如：给进程分配 3 个物理页面，逻辑页面的访问顺序为 1、2、3、4、5、6、1、2、3……

  - Clock 是 LRU、FIFO 的折中

    ​	页面访问时，不动态调整页面在链表中的顺序，仅做标记缺页时，再把它移动到链表末尾

    ​	对于被访问过的页面，Clock 不能记录准确访问顺序，而LRU算法可以

## 全局页面置换算法

- Belady现象

  在采用 FIFO 算法时，有时会出现分配的物理页面数增加，缺页率反而提高的异常现象

  出现原因：FIFO 算法的置换特征，与进程访问内存的动态特征是矛盾的，与置换算法的目标是不一致的（即替换较少使用的页面），因此，被置换出去的页面不一定是进程不会访问的

  通过全局页面置换算法来优化

- 局部置换算法的问题：程序运行的过程中，需要物理页帧的数量不是固定的，不能动态分配

  思路：全局置换算法为进程分配可变数目的物理页面

  解决

  ​	进程在不同阶段的内存需求是变化的

  ​	分配给进程的内存也需要在不同阶段有所变化

  ​	全局置换算法需要确定分配给进程的物理页面数

- CPU 利用率与并发进程数存在相互促进和制约的关系

  进程数少时，提高并发进程数，可提高 CPU 利用率

  并发进程导致内存访问增加

  并发进程的内存访问会降低了访存的局部性特征

  局部性特征的下降会导致缺页率上升和 CPU 利用率下降

- 工作集置换算法

  - 工作集模型

    一个进程当前正在使用的逻辑页面集合，用一个二元函数 W(t, Δ) 表示

    t：是当前的执行时刻

    Δ：称为工作窗口，即一个定长的页面访问的时间窗口

    W(t, Δ) = 在当前时刻 t 之前的 Δ 时间窗口当中的所有页面所组成的集合（随着 t 的变化，该集合也在不断变化）

    ​		如窗口大小为5，刚访问了页面是1,2,2,3,3    那么工作集为 1,2,3

    |W(t, Δ)|：指工作集的大小，即页面数目

    工作集大小的变化：进程开始执行后，随着访问新页面逐步建立较稳定的工作集，当内存访问的局部性区域的位置大致稳定时，工作集大小也大致稳定；局部性区域的位置改变时, 工作集快速扩张和收缩过渡到下一个稳定值	

    常驻集：在当前时刻，进程实际驻留在内存当中的页面集合

  - 工作集、常驻集的关系

    工作集是进程在运行过程中固有的性质

    而常驻集取决于系统分配给进程的物理页面数目，以及所采用的页面置换算法

  - 缺页率、常驻集的关系

    常驻集与工作集交集越多时，缺页越少

    工作集发生剧烈变动（过渡）时，缺页较多

    进程常驻集大小达到一定数目后，再给它分配更多的物理页面，缺页率也不会明显下降

  - 置换算法

    工作集的页置换算法并不是只有发生缺页中断时才把页换出到外存。随着程序在执行，工作集的窗口会跟着挪动，如果页不在工作集窗口中就会把页换出。保证了常驻集能够动态变化

    例子

    ​	如窗口大小为5，刚访问了页面是1,2,2,3,3    那么工作集为 1,2,3

    ​	那么换出常驻集中除了1,2,3的其他页面

    实现

    ​	访存链表：维护窗口内的访存页面，窗口大小固定不变

    ​	访存时，换出不在工作集的页面，更新访存链表

    ​	缺页时，换入页面，更新访存链表

    优缺点

    ​	虽然增加了缺页中断，但是却可以有效地保留最近访问的页，提前换出内存中的物理页，给其他应用程序提供更多的内存空间，达到动态分配内存空间的目的。与此同时，由于大部分写硬盘操作是提前完成的，所以当发生缺页中断时，大多数情况只需要写内存操作就行。

- 缺页率页面置换算法PFF

  ​    在PFF算法中，工作窗口大小是可以变化的

  ​	在每个进程运行开始时，先根据程序大小分配一定数目的物理页面，在运行过程中再根据缺页率来调整窗口大小。缺页率过低说明需要的物理页面足够，需要减小工作集，过低则表示分配的物理页不够需要增大工作集，来进行动态地调整工作集窗口大小。

    缺页率 = 缺页次数 /内存访问次数    ， 也是 缺页次数关于程序执行时间的导数。

- 抖动问题

  如果分配给一个进程的物理页面太少，常驻集不能包含整个的工作集，那么进程容易造成很多的缺页中断，页面在内存与外存之	间频繁换入换出，从而使进程的运行速度变得很慢

  原因：随着驻留内存的进程数目增加，分配给每个进程的物理页面数不断就减小，缺页率不断上升

  所以操作系统需要选择一个适当的进程数目和进程需要的帧数，以便在并发水平和缺页率中保持一个平衡

## 用户态和内核态

- 程序运行的两种状态
  - 内核态
    - 可以访问计算机的任何数据和资源，不受限制，包括外围设备，比如网卡、硬盘等。处于内核态的 CPU 可以从一个程序切换到另外一个程序，并且占用 CPU 不会发生抢占情况。
  - 用户态
    - 只能受限的访问内存，且不允许访问外围设备，占用cpu的能力被剥夺，cpu资源可以被其他程序获取,不能直接访问操作系统内核结构和数据
  - 操作系统数据都是存放于系统空间的，用户态进程的数据是存放在用户空间的，分开来存放就是为了让系统的数据和用户的数据互不干扰，保证系统的稳定性，分开存放，管理上比较方便，并且对于两部分数据的访问就可以进行控制，避免用户态程序恶意修改操作系统的数据和结构
  - 限制不同程序的访问能力，防止它们获取别的程序的内存数据，或者获取外围设备的数据并发送到网络。某些程序和安全性不高，如果执行了这些CPU的某些指令，会导致系统崩溃，如：**设置时钟、清理内存、调整网络**等
- 切换
  - 系统调用
    - 用户进程主动要求切换到内核态的一种方式，用户进程通过系统调用申请操作系统提供的服务程序完成工作
  - 异常
    - 当CPU在执行运行在用户态的程序时，发现了某些事情不可知的异常，这是会触发由当前运行进程切换到处理此异常的内核相关程序中，也就到了内核态，比如缺页异常
  - 外围设备中断
    - 当外围设备完成用户请求的操作之后，会像CPU发出相应的中断信号，这时CPU会暂停执行下一条将要执行的指令，转而去执行中断信号的处理程序
    - 比如硬盘读写操作完成，系统会切换到硬盘读写的中断处理程序中执行后续操作等

## 中断处理过程

- 一、关中断
  处理器响应中断后，首先要保护 程序的现场状态，在保护现场过程中， CPU 不应该响应更高级中断源的中断请求 。否则 ，如果现场保 存不完整，在中断服务程序结束后，也就不能正确 地恢 复并继续执行 现行程序。

- 二、保存断点
  为了保证中断服务程序执行完 毕后能 正确 地返回到原来的程序，必须将原来程序的断点保存起来。断点可以压入堆栈，也可以存入 主存 的特 定单元中。

- 三、引出中断服务程序
  引出中 断服务程序的 实质就是取出中断服务程序的入口地址送入程序计数器（ PC） 。
  通常有两种方法寻址中断服务程序的入口地 址： 硬件向量法和 软件 查询法。 
       硬件向量法是通过硬件产生中断向量地址，再由中断向量地址找到中断服务程序的入口地址 。
       软 件查询法是用软件编程的办法寻找入口地址。

- 四、保存现场和屏蔽字
  进入中断服务程序后首先要保存现场，现场信息一般指的是程序状态字、中断屏蔽寄存器和CPU中某些寄存器的内容。

- 五、开中断
  这将允许更高级中断请求得到响应， 实现中断嵌套。

- 六、执行中断服务程序
  这是中断系统的核心。不同的中断请求会有不同的中断服务程序。

- 七、关中断
  保 证在恢复 现场和 屏蔽字时不被 中断。

- 八、恢复现场和屏蔽字
  将现场和屏蔽字恢复到原来的状态。

- 九、开中断及中断返回
  中断服务程序的最后一条指令提交通常是一条中断返回指令 ，使其返回到原 程序的 断点处，以便 继续 执行原程序。

## 缓冲区溢出

- 缓冲区溢出是指当计算机向缓冲区填充数据时超出了缓冲区本身的容量，溢出的数据覆盖在合法数据上。
- 危害
  - 程序崩溃，导致拒绝额服务
  - 跳转并且执行一段恶意代码
    - 覆盖了内存中的其他程序

- 造成缓冲区溢出的主要原因是程序中没有仔细检查用户输入。

## 概述

### 基本特征

- 并发

  - 宏观上在一段时间内能同时运行多个程序，微观上交替执行
  - 操作系统通过引入进程和线程，使得程序能够并发运行

- 共享

  - 系统中的资源可以被多个并发进程共同使用
  - (互斥共享和同时共享
  - 互斥共享的资源称为临界资源，例如打印机等，在同一时刻只允许一个进程访问，需要用同步机制来实现互斥访问

- 虚拟

  - 把一个物理实体转换为多个逻辑实体。

  - 时分复用技术
    - 让每个进程轮流占用处理器，每次只执行一小个时间片并快速切换
  - 空分复用技术
    - 将物理内存抽象为地址空间，每个进程都有各自的地址空间
    - 地址空间的页被映射到物理内存，地址空间的页并不需要全部在物理内存中，当使用到一个没有在物理内存的页时，执行页面置换算法，将该页从磁盘置换到内存中

- 异步

  - 进程不是一次性执行完毕，而是走走停停，以不可知的速度向前推进

### 基本功能

- 进程管理
  - 进程控制、进程同步、进程通信、死锁处理、处理机调度等
- 内存管理
  - 内存分配、地址映射、内存保护与共享、虚拟内存等
- 文件管理
  - 文件存储空间的管理、目录管理、文件读写管理和保护等
- 设备管理
  - 完成用户的 I/O 请求，方便用户使用各种设备，并提高设备的利用率
  - 主要包括缓冲管理、设备分配、设备处理、虛拟设备等

### 中断分类

- 外中断
  - 由 CPU 执行指令以外的事件引起，如 I/O 完成中断，表示设备输入/输出处理已经完成，处理器能够发送下一个输入/输出请求。此外还有时钟中断、控制台中断等
- 异常
  - 由 CPU 执行指令的内部事件引起，如非法操作码、地址越界、算术溢出等
- 陷入
  - 在用户程序中使用系统调用

## 进程

### 进程和线程

- 进程

  - 程序的一次运行过程
  - 进程控制块 (Process Control Block, PCB) 描述进程的基本信息和运行状态，所谓的创建进程和撤销进程，都是指对 PCB 的操作

- 线程

  - 轻量级的进程，一个进程中可以有多个线程，共享进程资源
  - 浏览器进程里面有很多线程，例如 HTTP 请求线程、事件响应线程、渲染线程等等，线程的并发执行使得在浏览器中点击一个新链接从而发起 HTTP 请求时，浏览器还可以响应用户的其它事件

- 区别

  - 进程是资源分配的基本单位，线程是独立调度的基本单位

  - 资源开销

    - 切换

      - 进程间切换开销大，线程间切换小
      - 进程有独立的代码和数据空间，同一类线程共享代码和数据空间，每个线程都有自己独立的运行栈和程序计数器

- 创建和撤销
      - 进程：分配或回收资源，如内存空间、I/O 设备等，还要保存当前进程的进度
      - 线程：保存和设置少量寄存器内容

### 进程状态转换

- 新建，就绪，阻塞，运行，结束

![进程状态切换](B:/我的坚果云/面试题/images/进程状态切换.png)

- 细节
  - 只有就绪态和运行态可以相互转换，就绪态获得时间片进入运行态，运行态用完时间片后没结束就回到就绪态

### 进程调度算法

- 批处理系统：较少的用户操作，在该系统中，调度算法目标是保证吞吐量和周转时间（从提交到终止的时间）
  - 先来先服务 first-come first-serverd（FCFS）
    - 非抢占式，按照请求的顺序进行调度
    - 有利于长作业，但不利于短作业
  - 短作业优先 shortest job first（SJF）
    - 非抢占式，按估计运行时间最短的顺序进行调度
    - 有利于短作业，但不利于长作业
  - 最短剩余时间优先 shortest remaining time next（SRTN）
    - 抢占式，按剩余运行时间的顺序进行调度
    - 有新的短作业来，会挂起当前作业，运行更短的新作业
- 交互式系统：有大量的用户交互操作，在该系统中调度算法的目标是快速地进行响应
  - 优先级调度
    - 为每个进程分配一个优先级，按优先级进行调度
    - 为了防止低优先级的进程等不到调度，可随着时间的推移增加等待进程的优先级
  - 时间片轮转
    - 按先后顺序放入队列，队首进程运行一个时间片后放到队尾
    - 时间片太小，切换频繁；太大，实时性差
  - 多级反馈队列（优先级和时间片的结合）
    - 多个队列，每个队列的时间片大小不一样，时间片小的队列优先执行
    - 新来进程先进入时间片小的队列，运行一个时间片后移到时间大点的队列

- 实时系统
  - 要求一个请求在一个时间内得到响应
  - 分为硬实时和软实时，前者必须满足绝对的截止时间，后者可以容忍一定的超时

### 进程同步

- 临界区
  - 对临界资源进行访问的那段代码称为临界区
  - 为了互斥访问，进程在进入临界区之前要进行检查
- 互斥和同步
  - 互斥：多个进程在同一时刻只有一个进程能进入临界区
  - 同步：多个进程因为合作产生的直接制约关系，使得进程有一定的先后执行关系
- 信号量
  - （Semaphore）是一个整型变量，可对其执行 down 和 up 操作，也就是 P 和 V 操作
    - down: 如果信号量大于 0 ，执行 -1 操作；如果信号量等于 0，进程睡眠，等待信号量大于 0
    - up:对信号量执行 +1 操作，可唤醒睡眠的进程

```c++
typedef int semaphore;
semaphore mutex = 1;
void P() {
    down(&mutex);
    // 临界区
    up(&mutex);
}
```

- 管程
  - 信号量机制做很多控制，管程把控制的代码独立出来，不仅不容易出错，也使得客户端代码调用更容易
  - 在一个时刻只能有一个进程使用管程,**wait()** 和 **signal()** 来实现同步操作

### 生产者和消费者

- 互斥
- 使用一个缓冲区来保存物品，只有缓冲区没有满，生产者才可以放入物品；只有缓冲区不为空，消费者才可以拿走物品
- 信号量

```c++
#define N 100
typedef int semaphore;
semaphore mutex = 1;
semaphore empty = N;
semaphore full = 0;      //full和empty2个才好控制

void producer() {
    while(TRUE) {
        int item = produce_item();
        down(&empty);      //-1    P
        down(&mutex);
        insert_item(item);
        up(&mutex);
        up(&full);        //+1     V
    }
}

void consumer() {
    while(TRUE) {
        down(&full);
        down(&mutex);
        int item = remove_item();
        consume_item(item);
        up(&mutex);
        up(&empty);
    }
}
```

- 管程
  - C语言不支持管程，用了类 Pascal 语言来描述

```pascal
// 管程
monitor ProducerConsumer
    condition full, empty;
    integer count := 0;
    condition c;

    procedure insert(item: integer);
    begin
        if count = N then wait(full);
        insert_item(item);
        count := count + 1;
        if count = 1 then signal(empty);
    end;

    function remove: integer;
    begin
        if count = 0 then wait(empty);
        remove = remove_item;
        count := count - 1;
        if count = N -1 then signal(full);
    end;
end monitor;

// 生产者客户端
procedure producer
begin
    while true do
    begin
        item = produce_item;
        ProducerConsumer.insert(item);
    end
end;

// 消费者客户端
procedure consumer
begin
    while true do
    begin
        item = ProducerConsumer.remove;
        consume_item(item);
    end
end;
```

### 哲学家进餐

- 一张圆桌上坐着五名哲学家，在每两名哲学家中间放着一根筷子，哲学家们的生活方式只做两件事：思考和进餐。饥饿时哲学家必须同时拿起两只筷子时才能进餐，进餐完毕后，放下筷子，进行思考。如果筷子被紧挨着的一名哲学家使用着，则不能争抢，必须等待，当这名哲学家就餐完毕后，放下筷子，才能使用。如何让哲学家都吃到饭，而不发生饥饿现象（即不造成死锁）

- 解决方案
  - 至多允许4名哲学家进餐
  - 当左右筷子都可拿时再进餐，否则放弃拿筷子
  - 奇数哲学家先拿到左筷子，偶数哲学家先拿右筷子

### 读者写者问题

- 可多个进程写，但只能一个进程读
- count记录读进程数量，互斥量 count_mutex 用于对 count 加锁，互斥量 data_mutex 用于对读写加锁

```c++
typedef int semaphore;
semaphore count_mutex = 1;
semaphore data_mutex = 1;
int count = 0;

void reader() {
    while(TRUE) {
        down(&count_mutex);
        count++;
        if(count == 1) down(&data_mutex); // 第一个读者需要对数据进行加锁，防止写进程访问
        up(&count_mutex);
        read();
        down(&count_mutex);
        count--;
        if(count == 0) up(&data_mutex); //
        up(&count_mutex);
    }
}

void writer() {
    while(TRUE) {
        down(&data_mutex);
        write();
        up(&data_mutex);
    }
}
```

### 进程间通信IPC

- 无名管道pipe

  - 通过调用 pipe 函数创建的，fd[0] 用于读，fd[1] 用于写

  ```c++
  #include <unistd.h>
  int pipe(int fd[2]);
  ```

  - 特点
    - 只支持半双工通信（单向交替传输）
    - 没有名字，只能在父子进程或者兄弟进程中使用

  ![管道通信](B:/我的坚果云/面试题/images/管道通信.png)

- 有名管道fifo

  - 与文件系统中文件关联，数据先进先出

  ```c++
  #include <sys/stat.h>
  int mkfifo(const char *path, mode_t mode);
  int mkfifoat(int fd, const char *path, mode_t mode);
  ```

  - 特点
    - 也是半双工通信
    - 有名字，可用于任意进程间通信

- 消息队列message queue

  - 相比于 FIFO，有以下优点
    - 消息队列可以独立于读写进程存在，从而避免了 FIFO 中同步管道的打开和关闭时可能产生的困难
    - 避免了 FIFO 的同步阻塞问题，不需要进程自己提供同步方法
    - 读进程可以根据消息类型有选择地接收消息，而不像 FIFO 那样只能默认地接收

- 信号量

  - 它是一个计数器，用于为多个进程提供对共享数据对象的访问

- 共享存储

  - 多个进程可以将同一个文件映射到它们的地址空间从而实现共享内存

- 套接字

  - 与其它通信机制不同的是，它可用于不同机器间的进程通信

## 死锁

### 必要条件

- 互斥：资源互斥访问
- 占有和请求：进程占用资源并请求新的资源
- 不可抢占：已分配的资源不能被其他进程抢占
- 环路等待：2个及以上的进程形成等待环路

### 鸵鸟策略

- 把头埋在沙子里，假装根本没发生死锁
- 当发生死锁时不会对用户造成多大影响，或发生死锁的概率很低，可以采用鸵鸟策略

### 死锁检测

- 同类型资源只有一个
  - 检测有向图是否存在环，可DFS,对访问过的进程进行标记，若访问了已经标记的进程，则死锁
- 同类资源有多个
  - 银行家算法，试着让拿够资源的先运行，都能运行完则没有死锁

### 死锁恢复

- 破坏必要条件其中之一即可

## 内存管理

### 虚拟内存

- 概念
  - 为了更好的管理内存，操作系统将内存抽象成地址空间。每个程序拥有自己的地址空间，这个地址空间被分割成多个块，每一块称为一页
  - 这些页被映射到物理内存，但不需要映射到连续的物理内存，也不需要所有页都必须在物理内存中
  - 发生缺页中断时（访问的数据不在内存时），由硬件执行必要的映射，将缺失的部分装入物理内存并重新执行失败的指令
- 意义
  - 为了让物理内存扩充成更大的逻辑内存，从而让程序获得更多的可用内存
  - 虚拟内存允许程序不用将地址空间中的每一页都映射到物理内存，也就是说一个程序不需要全部调入内存就可以运行，这使得有限的内存运行大程序成为可能
- 实现方式
  - 请求分页存储管理
    - 建立在基本分页基础上的，为了能支持虚拟存储器功能而增加了请求调页功能和页面置换功能
  - 请求分段存储管理
  - 请求段页式存储管理

### 地址映射

- 内存管理单元（MMU）管理着地址空间和物理内存的转换

- 分页式
  - 页表（Page table）存储着页（程序地址空间）和页框（物理内存空间）的映射表
  - 虚拟地址（也叫逻辑地址吧）包含页面号和偏移量，通过虚拟地址找到物理地址
  - 每页大小相同
- 分段式
  - 每个表分成段，一个段构成一个独立的地址空间
  - 每个段的长度可以不同，并且可以动态增长
- 段页式
  - 程序的地址空间划分成多个拥有独立地址空间的段，每个段上的地址空间划分成大小相同的页
  - 既拥有分段系统的·共享和保护，又拥有分页系统的虚拟内存功能
- 分段和分页的区别
  - 透明性：分页透明，分段需要程序员显式划分每个段
  - 地址空间的维度：分页是一维地址空间，分段是二维的？
  - 大小：页大小固定，段可变
  - 目标：
    - 分页主要用于实现虚拟内存，从而获得更大的地址空间
    - 分段主要是为了使程序和数据可以被划分为逻辑上独立的地址空间并且有助于共享和保护

## 设备管理

### 磁盘结构

- 盘面（Platter）：一个磁盘有多个盘面；
- 磁道（Track）：盘面上的圆形带状区域，一个盘面可以有多个磁道；
- 扇区（Track Sector）：磁道上的一个弧段，一个磁道可以有多个扇区，它是最小的物理储存单位，目前主要有 512 bytes 与 4 K 两种大小；
- 磁头（Head）：与盘面非常接近，能够将盘面上的磁场转换为电信号（读），或者将电信号转换为盘面的磁场（写）；
- 制动手臂（Actuator arm）：用于在磁道之间移动磁头；
- 主轴（Spindle）：使整个盘面转动。

![磁盘结构](B:/我的坚果云/面试题/images/磁盘结构.png)

### 磁盘调度算法

- 读写一个磁盘块的时间的影响因素有：
  - 旋转时间（主轴转动盘面，使得磁头移动到适当的扇区上）
  - 寻道时间（制动手臂移动，使得磁头移动到适当的磁道上）
  - 实际的数据传输时间
  - 寻道时间最长，因此磁盘调度的主要目标是使磁盘的平均寻道时间最短。
- 先来先服务（FCFS, First Come First Service
  - 按照磁盘请求的顺序进行调度
  - 优点是公平和简单。缺点也很明显，因为未对寻道做任何优化，使平均寻道时间可能较长
- 最短寻道时间优先（SSTF, Shortest Seek Time First
  - 优先调度与当前磁头所在磁道距离最近的磁道
  - 平均寻道时间比较低，但远的磁道可能会出现饥饿
- 电梯算法（SCAN
  - 总是按一个方向来进行磁盘调度，直到该方向上没有未完成的磁盘请求，然后改变方向

## 其他

### 编译过程

- 步骤
  - 预处理阶段：处理以 # 开头的预处理命令；
  - 编译阶段：翻译成汇编文件；
  - 汇编阶段：将汇编文件翻译成可重定位目标文件；
  - 链接阶段：将可重定位目标文件和 printf.o 等单独预编译好的目标文件进行合并，得到最终的可执行目标文件。
- 图

![编译过程](B:/我的坚果云/面试题/images/编译过程.png)

### 静动态链接

- 静态

  - 静态链接器以一组可重定位目标文件为输入，生成一个完全链接的可执行目标文件作为输出
  - 静态链接器
    - 符号解析：每个符号对应于一个函数、一个全局变量或一个静态变量，符号解析的目的是将每个符号引用与一个符号定义关联起来
    - 重定位：链接器通过把每个符号定义与一个内存位置关联起来，然后修改所有对这些符号的引用，使得它们指向这个内存位置

  ![静态链接](B:/我的坚果云/面试题/images/静态链接.png)

  - 存在问题
    - 当静态库更新时那么整个程序都要重新进行链接
    - 对于标准函数库，如果每个程序都要有代码，这会极大浪费资源

- 动态

  - 换成了动态链接库（共享库）
  - 特点
    - 在 Linux 系统中通常用 .so 后缀，Windows 上.ddl
    - 在给定的文件系统中一个库只有一个文件，所有引用该库的可执行目标文件都共享这个文件，它不会被复制到引用它的可执行文件中
    - 在内存中，一个共享库的 .text 节（已编译程序的机器代码）的一个副本可以被不同的正在运行的进程共享

### IO模型

- 在《[Unix](https://so.csdn.net/so/search?q=Unix&spm=1001.2101.3001.7020)网络编程》一书中提到了五种IO模型，分别是：阻塞IO、非阻塞IO、多路复用IO、信号驱动IO以及异步IO。

- 阻塞IO

  最传统的一种IO模型，即在读写数据过程中会发生阻塞现象。

  　　当用户线程发出IO请求之后，内核会去查看数据是否就绪，如果没有就绪就会等待数据就绪，而用户线程就会处于阻塞状态，用户线程交出CPU。当数据就绪之后，内核会将数据拷贝到用户线程，并返回结果给用户线程，用户线程才解除block状态。

  典型例子

  data = socket.read(); //  如果数据没有就绪，就会一直阻塞在read方法。

- 非阻塞IO

  当用户线程发起一个read操作后，并不需要等待，而是马上就得到了一个结果。如果结果是一个error时，它就知道数据还没有准备好，于是它可以再次发送read操作。一旦内核中的数据准备好了，并且又再次收到了用户线程的请求，那么它马上就将数据拷贝到了用户线程，然后返回。

     所以事实上，在非阻塞IO模型中，用户线程需要不断地询问内核数据是否就绪，也就说非阻塞IO不会交出CPU，而会一直占用CPU。

- 多路复用IO

  多路复用IO模型是目前使用得比较多的模型。Java NIO实际上就是多路复用IO。

  　　在多路复用IO模型中，会有一个线程不断去轮询多个socket的状态，只有当socket真正有读写事件时，才真正调用实际的IO读写操作。因为在多路复用IO模型中，只需要使用一个线程就可以管理多个socket，系统不需要建立新的进程或者线程，也不必维护这些线程和进程，并且只有在真正有socket读写事件进行时，才会使用IO资源，所以它大大减少了资源占用。

  　　在Java NIO中，是通过selector.select()去查询每个通道是否有到达事件，如果没有事件，则一直阻塞在那里，因此这种方式会导致用户线程的阻塞。

  　　也许有朋友会说，我可以采用 多线程+ 阻塞IO 达到类似的效果，但是由于在多线程 + 阻塞IO 中，每个socket对应一个线程，这样会造成很大的资源占用，并且尤其是对于长连接来说，线程的资源一直不会释放，如果后面陆续有很多连接的话，就会造成性能上的瓶颈。

  　　而多路复用IO模式，通过一个线程就可以管理多个socket，只有当socket真正有读写事件发生才会占用资源来进行实际的读写操作。因此，多路复用IO比较适合连接数比较多的情况。

  　　另外多路复用IO为何比非阻塞IO模型的效率高是因为在非阻塞IO中，不断地询问socket状态是通过用户线程去进行的，而在多路复用IO中，轮询每个socket状态是内核在进行的，这个效率要比用户线程要高的多。

  　　不过要注意的是，多路复用IO模型是通过轮询的方式来检测是否有事件到达，并且对到达的事件逐一进行响应。因此对于多路复用IO模型来说，一旦事件响应体很大，那么就会导致后续的事件迟迟得不到处理，并且会影响新的事件轮询

- 信号驱动IO

  在信号驱动IO模型中，当用户线程发起一个IO请求操作，会给对应的socket注册一个信号函数，然后用户线程会继续执行，当内核数据就绪时会发送一个信号给用户线程，用户线程接收到信号之后，便在信号函数中调用IO读写操作来进行实际的IO请求操作

- 异步IO

  异步IO模型才是最理想的IO模型，在异步IO模型中，当用户线程发起read操作之后，立刻就可以开始去做其它的事。而另一方面，从内核的角度，当它收到一个asynchronous read之后，它会立刻返回，说明read请求已经成功发起了，因此不会对用户线程产生任何block。然后，内核会等待数据准备完成，然后将数据拷贝到用户线程，当这一切都完成之后，内核会给用户线程发送一个信号，告诉它read操作完成了。也就说用户线程完全不需要知道实际的整个IO操作是如何进行的，只需要先发起一个请求，当接收内核返回的成功信号时表示IO操作已经完成，可以直接去使用数据了。

  　　也就说在异步IO模型中，IO操作的两个阶段都不会阻塞用户线程，这两个阶段都是由内核自动完成，然后发送一个信号告知用户线程操作已完成。用户线程中不需要再次调用IO函数进行具体的读写。这点是和信号驱动模型有所不同的，在信号驱动模型中，当用户线程接收到信号表示数据已经就绪，然后需要用户线程调用IO函数进行实际的读写操作；而在异步IO模型中，收到信号表示IO操作已经完成，不需要再在用户线程中调用iO函数进行实际的读写操作。

### IO模型简洁版

- 同步IO

  ​	同步 io 是用户线程发起 io 请求并以阻塞或轮询的方式来等待 io 的完成
  ​	同步 io 是 io 的发起方，同时也是处理方
  ​	同步 io 是需要将内核态准备就绪的数据拷贝到用户态，所以需要阻塞用户态程序并等待 io 完成

- 异步IO

  ​	异步 io 在用户线程发起 io 请求后会立即返回继续执行后续的逻辑流
  ​	异步 io 是 io 的发起方，但内核态才是处理方
  ​	异步 io 的处理方是内核态，所以不需要阻塞

- 阻塞IO

  用户线程发起 io 请求并阻塞用户线程释放 CPU 执行权，等待内核态的 io 处理完成

- 非阻塞IO

  用户线程发起 io 请求会立即返回，处理后面的代码，但是会有（额外的）线程以轮询的方式查询内核态的 io 是否处理完成，如果 io 完成则立即拷贝到用户进程，这种方式对 CPU 资源消耗较高

- 多路复用IO

  Java nio 就是多路复用的 io 模型，多路复用模型是由一个线程监听多个 socket，这种方案比较适用于 io 比较多的情况，io 多路复用的性能是比非阻塞 io 要高的，因为多路复用模型的轮询是在内核态，而非阻塞 io 的轮询是在用户态，但是在任务数量比较多或比较大的情况下 io 多路复用需要逐一去处理已完成的 io，会导致后续的 io 得不到处理或者等待过长时间才能得到处理。

- 信号驱动IO

  用户线程发起 io 请求，然后给负责 io 的 socket 注册一个函数用于完成后的回调，当内核态数据准备完成后会发出一个信号，用户线程接收后会调用之前注册的函数来读写 io

- 异步IO

  用户线程发起 io 请求后会立即返回，并当内核态的 io 完毕后会将数据拷贝到用户态，然后再发送信号通知用户线程已就绪，整个过程用户线程是不会阻塞也不需要其它额外操作的，除了发起 io 请求，处理 io 和拷贝数据均由内核态完成。

### select、poll 和 epoll 

- 多路复用IO的实现方式

- select

  ```c
  int select(int maxfdp1,fd_set *readset,fd_set *writeset,fd_set *exceptset,const struct timeval *timeout)
  有文件描述符就绪时，返回就绪数量>1,超时时返回0，异常返回-1
  12
  ```

   在select模式下它有一个select方法，有5个参数，首先是**待监测文件描述的数量**；中间三个参数是我们想让内核监测的**读、写、异常事件的文件描述集合fd_set**，如果不想监测该类事件直接传null即可；最后是一个超时时间的结构体，可以设置等待的秒数和微秒数，**告知内核如果没有文件描述准备就绪的话，等待多久再去重新扫描fd_set(即我们监测的文件描述集合)**，它可以指定三种模式：

  (1) 设为null，如果没有文件描述IO准备就绪的话，那么一直等待

  (2) 设置固定的时间，等待一段时间后如果没有文件描述IO准备就绪，那么返回0，之后会重新轮询一遍fd_set如果没有文件描述准备好，那么再次等待，之后会重复这一过程

  (3) 根本不等待，扫描一遍fd_set后，立即返回，重新再扫描即轮询，为此超时时间结构体不能为空，需要把等待时间参数设为0

    select每次轮询时，都需要把文件描述从用户空间复制到内核空间，具体原因我认为是为了避免文件描述有新加入或断开的情况，但是的这一过程会比较消耗资源；同时内核在等待时，发生文件描述IO事件准备就绪时，虽然能够返回就绪事件数量，但是并不知道是哪个文件描述就绪，需要再次轮询，如果监测的fd很多的话会比较耗时；select是以数组的形式存储的待监测fd，长度有限制默认为1024；最后就是**select是水平触发的方式**，**如果用户线程没有处理准备就绪的文件事件，那么下次select调用还会将这个事件进行返回**

- poll

  ```c
  int poll ( struct pollfd * fds, unsigned int nfds, int timeout);
  ```

    poll方法比较简单，由一个结构体pollfd存储待监测文件描述，nfds表示待监测fd数量，最后有一个超时时间，当timeout=0时表示轮询遍历；大于0时，表示当有fd IO事件准备就绪或到达等待事件时返回；小于0时表示，如果没有fd准备就绪那么就一直等待。

    poll与select运行模式比较类似，开始工作时都会先轮询一遍待监测fd集合，**之后根据等待时间陷入等待，当期间有文件描述IO事件准备就绪或者超时时返回**，继续轮询fd集合，每次轮询时都需要将fd集合从用户空间复制到内核空间。**与select不同的是poll使用pollfd存储存储待监测fd，select使用的是fd_set，pollfd是使用链表实现的，不会再限制监测fd的数量。**

    **select与poll在等待的过程中如果发生了fd IO就绪事件会立即返回，但是并不知道是哪个fd准备就绪，需要再次轮询。**

- epoll

  ```c
  int epoll_create(int size);
  int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);
  int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout);
  ```

  epoll中有三个方法，

    (1)create会创建epoll句柄即监听集合，size表示它监听的描述符数量，能打开的监听数量远超select，单cpu可打开连接数量超过10W；

    (2)ctl方法为fd注册函数，向epfd中注册待监听fd，epfd表示create方法的返回值，将要注册到哪个句柄；op表示动作，是向epfd中添加fd，删除或修改已注册fd的监测事件；fd为要被监测的文件描述；event为该fd需要被监测的事件类型；

    (3)最终通过调用wait方法等待事件产生，获取IO事件准备就绪的fd。events为就绪事件描述，maxevents为事件最大数量，最后为超时时间。

  优点：

    (1)**epoll是基于事件驱动模式，相较于select来说，它的文件描述集合数量远大于1024，单cpu可支持连接数约为10万；**

    (2)**基于事件回调**，当fd调用epoll_ctl方法加入监听集合的时候，就会注册回调，当事件准备就绪后，进入一个就绪队列中，**在调用wait方法时不再用遍历整个fd监测集合，只需要判断一下就绪队列是否为空就行了；**

    (3)此外每次注册新fd调用epoll_ctl的过程中就会把fd从用户空间复制进内核空间进行存储或删除，保证只复制一次，**不用每次调用wait方法时都需要从用户空间复制fd到内核空间。**

    此外epoll除了支持select、poll的**水平触发之外还支持边缘触发**，当epoll_wait检测到某描述符事件就绪并通知应用程序时，应用程序必须立即处理该事件，如果不处理，下次调用epoll_wait时，不会再次通知此事件。

- 对比

  - select的调用复杂度O(n)。如一个保姆照看一群孩子，如果把孩子是否需要吃饭比作网络I/O事件，select就像保姆挨个询问每个孩子：你要吃饭吗？若孩子回答是，保姆则把孩子拎出来放到另外一个地方。当所有孩子询问完之后，保姆领着这些要吃饭的孩子去用餐（处理网络I/O事件）

  - epoll机制下，保姆无需挨个询问孩子是否要吃饭，而是每个孩子若自己需要吃饭，主动站到事先约定好的地方，而保姆职责就是查看事先约定好的地方是否有孩子。若有小孩，则领着孩子去上吃饭（网络事件处理）。因此，epoll的这种机制，能够高效的处理成千上万的并发连接，而且性能不会随着连接数增加而下降。
